# PaperToPyTorch
### ğŸ” ë…¼ë¬¸ì˜ ì´ë¡ ì„ PyTorchë¡œ êµ¬í˜„í•˜ë©°, ë”¥ëŸ¬ë‹ ì‹¤ìŠµê³¼ ì§€ì‹ì„ ìŒ“ëŠ” ê³µê°„ì…ë‹ˆë‹¤.   
   * ê°€ëŠ¥í•œ ìµœì‹  ë…¼ë¬¸ì„ íƒêµ¬í•˜ë©°, ìµœì‹  ë…¼ë¬¸ì„ ì´í•´í•˜ëŠ”ë° í•„ìš”í•œ ë…¼ë¬¸ ë˜í•œ í•¨ê»˜ ë¦¬ë·°í•˜ê³  êµ¬í˜„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

#### Multimodal Learning (ë©€í‹°ëª¨ë‹¬ í•™ìŠµ)
* CLIP : Learning Transferable Visual Models From Natural Language Supervision (ICML 2021)
    * [Original Paper Link](https://arxiv.org/abs/2103.00020)
       
#### Computer Vision (ì»´í“¨í„° ë¹„ì „)
* SimCLR: A Simple Framework for Contrastive Learning of Visual Representations (ICML 2020)
    * [Original Paper Link](https://arxiv.org/abs/2002.05709)

#### Natural Language Processing (ìì—°ì–´ ì²˜ë¦¬)
* Attention is All You Need (NIPS 2017)
    * [Original Paper Link](https://arxiv.org/abs/1706.03762)
* Mamba: Linear-Time Sequence Modeling with Selective State Spaces (2023)
    * [Original Paper Link](https://arxiv.org/abs/2312.00752)   

#### Large Language Model (ëŒ€í˜• ì–¸ì–´ ëª¨ë¸, LLM)
* The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits (2024)
    * [Original Paper Link](https://arxiv.org/abs/2103.00020)
